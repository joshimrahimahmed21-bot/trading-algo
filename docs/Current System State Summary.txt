Developer Handbook & Best Practices (Updated)
•	Modular Design & Partial Classes: Continue to use partial class files to organize strategy logic by feature (e.g. EntryQuality, MomentumPosVol, SizingRunner). This keeps code maintainable and allows incremental enhancements without monolithic functions. For example, the momentum core and positional volume logic are stubbed in a partial (MNQRSTest_MomentumPosVol.cs) so that future expansion (like integrating a full node graph) can be done in isolation. New features such as Relativity/Normalization nodes should follow this pattern – implement them as separate methods or partials, and integrate via clearly named toggles/parameters (e.g. UseRelativity, UsePosVolNodes) for easy enable/disable.
•	Node Graph Approach: Embrace a node-based signal processing approach for quality metrics. We are now wiring in Relativity (normalization) and Decay nodes for signals like PosVol and momentum. Best practice is to treat these as modular components: each node (e.g. a Z-score normalizer or an EWMA decayer) should have its own state and parameters. This means avoiding “hard-coded” logic in one big method; instead, encapsulate in functions like RelativityNode_Update() or DecayNode_Update() that handle their internal memory (running mean, decay accumulator, etc.). By doing so, we maintain clean code where each signal’s transformation is pluggable. It also makes testing easier – e.g. one can unit-test that the Relativity node outputs a z-score on a test series.
•	Parameter Management: With the introduction of new signal processing, add corresponding parameters for flexibility. For instance, if we implement a rolling Z-score, include a parameter for the window length or decay factor used in normalization. Maintain the convention of grouping parameters logically (as done with GroupName “Parameters”, “Entry Filters”, etc.). Ensure new parameters have sensible defaults. A best practice is to use realistic default values now that we have more insight: for example, if adding RelativityWindow (bars for normalization) default it to a moderate number (e.g. 100 bars) rather than 0 or an extreme. Similarly, if enabling the volatility filter by default, set its MinATR/MaxATR to reasonable values reflecting typical market behavior so the strategy isn’t accidentally filtering out all trades or none – e.g. for MNQ, a MinATR around a few points might make sense instead of 0.0. We’ve learned to avoid placeholder defaults like 0 which can disable a feature without the user realizing; instead, use defaults that activate the feature in a mild, sane way (and the user can turn it off if needed).
•	Logging and Performance: As the strategy grows more complex and we plan to run large optimization sweeps, be mindful of logging overhead. We discovered that heavy file logging (writing every setup/trade/exit) can significantly slow down backtests and consume disk space. A best practice is to include a master switch or mode (e.g. ExportDuringOptimization or setting LogSetupsVerbose=false) that minimizes logging when running mass optimizations. For example, for overnight sweeps with thousands of iterations, consider logging only high-level metrics or using an in-memory summary instead of per-trade files. We have a pattern for this: the code’s LogSetupsVerbose already controls detailed setup logs – ensure to utilize this flag (and similar ones) to silence non-critical logs during brute-force optimizations. In the same vein, code any new debug prints or diagnostic info behind conditionals, so they can be turned off easily. This will keep the strategy performant when scaling up the number of runs.
•	NinjaTrader Idiosyncrasies: Continue accounting for NT8 quirks in our coding style. We already adjusted for the DefaultQuantity limitation by introducing our own BaseContracts property – any further work should respect NinjaTrader’s constraints. For example, if implementing dynamic risk or scaling, remember that changing DefaultQuantity at runtime is not possible; instead, plan to adjust BaseContracts between trades or in OnBarUpdate logic when flat. Another NT8 quirk is the inability to modify existing orders’ quantities – our approach to runner scaling (adjusting an internal lastRunnerPct each bar based on volume profile) cannot actually change the live position size once in trade. Best practice here is transparency: document these limitations in code comments and ensure any feature that appears to modify active trades either uses supported methods (like submitting new orders or stops) or is clearly for analysis only. For instance, we use ApplyVPManagementAdjustments() to tweak an internal metric rather than truly adjusting the order – this is acceptable as long as it’s well-communicated that it’s an informational or next-trade adjustment, due to platform constraints. Future developers should approach similar challenges by either finding NT8-supported workarounds or explicitly notating that a certain adjustment cannot be live-applied and is experimental.
•	Robustness & Safety: As we approach deployment, prioritize fail-safes. Incorporate kill-switches and protective checks in the code. For example, if too many errors occur or a certain drawdown is hit, the strategy should ideally stop taking new trades (this might be implemented via IsSuspended flag or simply setting Enabled = false on the strategy from code). Another best practice is to handle exceptions around any indicator or I/O operation – e.g. our use of RollingStats for z-scores or any file writing should be wrapped in try/catch to prevent a single glitch from terminating the strategy mid-run. We’ve done this in places like the logging code (catching exceptions around file writes); continue this pattern for new features (e.g., if a volume profile indicator returns unexpected values or our relativity calc gets a divide-by-zero, handle it gracefully). Also, ensure to test edge cases (like very low volume bars, or no trades in a session) so that the strategy doesn’t enter an undefined state. Finally, keep the code clean and commented: as we add complexity (relativity nodes, regime logic, dynamic sizing rules), maintain clear comments and XML summaries for each method, as currently done. This helps any new developer (or our future selves) understand the rationale behind each component, especially for intricate logic like “TrueMomo blending” or “regime-based TP adjustment” which can be non-intuitive without context.
